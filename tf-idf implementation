# Import required libraries
import chardet
import math
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

#  Detect the encoding of a file
def find_encoding(fname):
    r_file = open(fname, 'rb').read()
    result = chardet.detect(r_file)
    charenc = result['encoding']
    return charenc

# Data pre-processing: stopwords removal and removal of any leading numbers from the string
def preprocess(document_text):
    stopwords_set = set(stopwords.words('english'))
    terms = [term.lower() for term in document_text.strip().split()[1:] if term.lower() not in stopwords_set]
    return terms

# Read the input file, preprocess the data, build the vocabulary, and construct the inverted index.
def load_data():
    vocab = {}
    documents = []
    inverted_index = {}

    filename = '/Qdatalc/indexlc.txt'
    my_encoding = find_encoding(filename)

    with open(filename, 'r', encoding=my_encoding) as f:
        lines = f.readlines()
    
    for index, line in enumerate(lines):
        tokens = preprocess(line)
        documents.append(tokens)
        for token in tokens:
            vocab[token] = vocab.get(token, 0) + 1
            if token not in inverted_index:
                inverted_index[token] = [index]
            else:
                inverted_index[token].append(index)

    vocab = dict(sorted(vocab.items(), key=lambda item: item[1], reverse=True))
    save_data(vocab, documents, inverted_index)

    return vocab, documents, inverted_index

# Save the vocabulary, IDF values, documents, and inverted index into separate text files
def save_data(vocab, documents, inverted_index):
    # Save the vocab in a text file
    with open('tf-idf_lc/vocab_lc.txt', 'w') as f:
        for key in vocab.keys():
            f.write("%s\n" % key)

    # Save the idf values in a text file
    with open('tf-idf_lc/idf-values_lc.txt', 'w') as f:
        for key in vocab.keys():
            f.write("%s\n" % vocab[key])

    # Save the documents in a text file
    with open('tf-idf_lc/documents_lc.txt', 'w') as f:
        for document in documents:
            f.write("%s\n" % ' '.join(document))
            
    # Save the inverted index in a text file
    with open('tf-idf_lc/inverted-index_lc.txt', 'w') as f:
        for key in inverted_index.keys():
            f.write("%s\n" % key)
            f.write("%s\n" % ' '.join([str(doc_id) for doc_id in inverted_index[key]]))

vocab_idf_values, documents, inverted_index = load_data()

# Load the vocabulary and IDF values from the text files
def load_vocab():
    vocab = {}
    with open('tf-idf_lc/vocab_lc.txt', 'r') as f:
        vocab_terms = f.readlines()
    with open('tf-idf_lc/idf-values_lc.txt', 'r') as f:
        idf_values = f.readlines()
    
    for (term, idf_value) in zip(vocab_terms, idf_values):
        vocab[term.strip()] = int(idf_value.strip())
    
    return vocab

# Load the preprocessed documents from the text file
def load_documents():
    documents = []
    with open('tf-idf_lc/documents_lc.txt', 'r') as f:
        documents = f.readlines()
    documents = [document.strip().split() for document in documents]

    print('Number of documents:', len(documents))
    print('Sample document:', documents[0])
    return documents

# Load the inverted index from a text file
def load_inverted_index():
    inverted_index = {}
    with open('tf-idf_lc/inverted-index_lc.txt', 'r') as f:
        inverted_index_terms = f.readlines()

    for row_num in range(0, len(inverted_index_terms), 2):
        term = inverted_index_terms[row_num].strip()
        documents = inverted_index_terms[row_num + 1].strip().split()
        inverted_index[term] = documents
    
    print('Size of inverted index:', len(inverted_index))
    return inverted_index
    
# Calculate term frequency
def get_tf_dictionary(term):
    tf_values = {}
    if term in inverted_index:
        for document in inverted_index[term]:
            if document not in tf_values:
                tf_values[document] = 1
            else:
                tf_values[document] += 1
                
    for document in tf_values:
        tf_values[document] /= len(documents[int(document)])
    
    return tf_values

# Calculate inverse document frequency
def get_idf_value(term):
    return math.log(len(documents) / vocab_idf_values[term])

# # Calculate the relevance scores for the given query terms and retrieve the matching documents
def calculate_sorted_order_of_documents(query_terms):
    potential_documents = {}
    for term in query_terms:
        if term in vocab_idf_values and vocab_idf_values[term] != 0:
            tf_values_by_document = get_tf_dictionary(term)
            idf_value = get_idf_value(term)
            print(term, tf_values_by_document, idf_value)
            for document in tf_values_by_document:
                if document not in potential_documents:
                    potential_documents[document] = tf_values_by_document[document] * idf_value
                else:
                    potential_documents[document] += tf_values_by_document[document] * idf_value

    if not potential_documents:
        print("No matching documents found.")

    else:
        # divide by the length of the query terms
        for document in potential_documents:
            potential_documents[document] /= len(query_terms)
            
        potential_documents = dict(sorted(potential_documents.items(), key=lambda item: item[1], reverse=True))

        for document_index in potential_documents:
            print('Document: ', documents[int(document_index)], ' Score: ', potential_documents[document_index])

query_string = input('Enter your query: ')
query_terms = [term.lower() for term in query_string.strip().split()]

print(query_terms)
calculate_sorted_order_of_documents(query_terms)
